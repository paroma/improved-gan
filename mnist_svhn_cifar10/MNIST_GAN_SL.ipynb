{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import theano as th\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import lasagne.layers as LL\n",
    "import time\n",
    "import nn\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=100, count=10, seed=1, seed_data=1, unlabeled_weight=1.0)\n"
     ]
    }
   ],
   "source": [
    "# settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=1)\n",
    "parser.add_argument('--seed_data', type=int, default=1)\n",
    "parser.add_argument('--unlabeled_weight', type=float, default=1.)\n",
    "parser.add_argument('--batch_size', type=int, default=100)\n",
    "parser.add_argument('--count', type=int, default=10)\n",
    "args = parser.parse_args([])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fixed random seeds\n",
    "rng = np.random.RandomState(args.seed)\n",
    "theano_rng = MRG_RandomStreams(rng.randint(2 ** 15))\n",
    "lasagne.random.set_rng(np.random.RandomState(rng.randint(2 ** 15)))\n",
    "data_rng = np.random.RandomState(args.seed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# specify generative model\n",
    "noise = theano_rng.uniform(size=(args.batch_size, 100))\n",
    "gen_layers = [LL.InputLayer(shape=(args.batch_size, 100), input_var=noise)]\n",
    "gen_layers.append(nn.batch_norm(LL.DenseLayer(gen_layers[-1], num_units=500, nonlinearity=T.nnet.softplus), g=None))\n",
    "gen_layers.append(nn.batch_norm(LL.DenseLayer(gen_layers[-1], num_units=500, nonlinearity=T.nnet.softplus), g=None))\n",
    "gen_layers.append(nn.l2normalize(LL.DenseLayer(gen_layers[-1], num_units=28**2, nonlinearity=T.nnet.sigmoid)))\n",
    "gen_dat = LL.get_output(gen_layers[-1], deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# specify supervised model\n",
    "layers = [LL.InputLayer(shape=(None, 28**2))]\n",
    "layers.append(nn.GaussianNoiseLayer(layers[-1], sigma=0.3))\n",
    "layers.append(nn.DenseLayer(layers[-1], num_units=1000))\n",
    "layers.append(nn.GaussianNoiseLayer(layers[-1], sigma=0.5))\n",
    "layers.append(nn.DenseLayer(layers[-1], num_units=500))\n",
    "layers.append(nn.GaussianNoiseLayer(layers[-1], sigma=0.5))\n",
    "layers.append(nn.DenseLayer(layers[-1], num_units=250))\n",
    "layers.append(nn.GaussianNoiseLayer(layers[-1], sigma=0.5))\n",
    "layers.append(nn.DenseLayer(layers[-1], num_units=250))\n",
    "layers.append(nn.GaussianNoiseLayer(layers[-1], sigma=0.5))\n",
    "layers.append(nn.DenseLayer(layers[-1], num_units=250))\n",
    "layers.append(nn.GaussianNoiseLayer(layers[-1], sigma=0.5))\n",
    "layers.append(nn.DenseLayer(layers[-1], num_units=10, nonlinearity=None, train_scale=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# costs\n",
    "labels = T.ivector()\n",
    "x_lab = T.matrix()\n",
    "x_unl = T.matrix()\n",
    "\n",
    "temp = LL.get_output(gen_layers[-1], init=True)\n",
    "temp = LL.get_output(layers[-1], x_lab, deterministic=False, init=True)\n",
    "init_updates = [u for l in gen_layers+layers for u in getattr(l,'init_updates',[])]\n",
    "\n",
    "output_before_softmax_lab = LL.get_output(layers[-1], x_lab, deterministic=False)\n",
    "output_before_softmax_unl = LL.get_output(layers[-1], x_unl, deterministic=False)\n",
    "output_before_softmax_fake = LL.get_output(layers[-1], gen_dat, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "z_exp_lab = T.mean(nn.log_sum_exp(output_before_softmax_lab))\n",
    "z_exp_unl = T.mean(nn.log_sum_exp(output_before_softmax_unl))\n",
    "z_exp_fake = T.mean(nn.log_sum_exp(output_before_softmax_fake))\n",
    "l_lab = output_before_softmax_lab[T.arange(args.batch_size),labels]\n",
    "l_unl = nn.log_sum_exp(output_before_softmax_unl)\n",
    "loss_lab = -T.mean(l_lab) + T.mean(z_exp_lab)\n",
    "loss_unl = -0.5*T.mean(l_unl) + 0.5*T.mean(T.nnet.softplus(nn.log_sum_exp(output_before_softmax_unl))) + 0.5*T.mean(T.nnet.softplus(nn.log_sum_exp(output_before_softmax_fake)))\n",
    "\n",
    "train_err = T.mean(T.neq(T.argmax(output_before_softmax_lab,axis=1),labels))\n",
    "\n",
    "mom_gen = T.mean(LL.get_output(layers[-3], gen_dat), axis=0)\n",
    "mom_real = T.mean(LL.get_output(layers[-3], x_unl), axis=0)\n",
    "loss_gen = T.mean(T.square(mom_gen - mom_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test error\n",
    "output_before_softmax = LL.get_output(layers[-1], x_lab, deterministic=True)\n",
    "test_err = T.mean(T.neq(T.argmax(output_before_softmax,axis=1),labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Theano functions for training and testing\n",
    "lr = T.scalar()\n",
    "disc_params = LL.get_all_params(layers, trainable=True)\n",
    "disc_param_updates = nn.adam_updates(disc_params, loss_lab + args.unlabeled_weight*loss_unl, lr=lr, mom1=0.5)\n",
    "disc_param_avg = [th.shared(np.cast[th.config.floatX](0.*p.get_value())) for p in disc_params]\n",
    "disc_avg_updates = [(a,a+0.0001*(p-a)) for p,a in zip(disc_params,disc_param_avg)]\n",
    "disc_avg_givens = [(p,a) for p,a in zip(disc_params,disc_param_avg)]\n",
    "gen_params = LL.get_all_params(gen_layers[-1], trainable=True)\n",
    "gen_param_updates = nn.adam_updates(gen_params, loss_gen, lr=lr, mom1=0.5)\n",
    "init_param = th.function(inputs=[x_lab], outputs=None, updates=init_updates)\n",
    "train_batch_disc = th.function(inputs=[x_lab,labels,x_unl,lr], outputs=[loss_lab, loss_unl, train_err], updates=disc_param_updates+disc_avg_updates)\n",
    "train_batch_gen = th.function(inputs=[x_unl,lr], outputs=[loss_gen], updates=gen_param_updates)\n",
    "test_batch = th.function(inputs=[x_lab,labels], outputs=test_err, givens=disc_avg_givens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load MNIST data\n",
    "data = np.load('mnist.npz')\n",
    "trainx = np.concatenate([data['x_train'], data['x_valid']], axis=0).astype(th.config.floatX)\n",
    "trainx_unl = trainx.copy()\n",
    "trainx_unl2 = trainx.copy()\n",
    "trainy = np.concatenate([data['y_train'], data['y_valid']]).astype(np.int32)\n",
    "nr_batches_train = int(trainx.shape[0]/args.batch_size)\n",
    "testx = data['x_test'].astype(th.config.floatX)\n",
    "testy = data['y_test'].astype(np.int32)\n",
    "nr_batches_test = int(testx.shape[0]/args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select labeled data\n",
    "inds = data_rng.permutation(trainx.shape[0])\n",
    "trainx = trainx[inds]\n",
    "trainy = trainy[inds]\n",
    "txs = []\n",
    "tys = []\n",
    "for j in range(10):\n",
    "    txs.append(trainx[trainy==j][:args.count])\n",
    "    tys.append(trainy[trainy==j][:args.count])\n",
    "txs = np.concatenate(txs, axis=0)\n",
    "tys = np.concatenate(tys, axis=0)\n",
    "\n",
    "init_param(trainx[:500]) # data dependent initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not enough arguments for format string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-26037209c2c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss_unl\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch %d, train err = %.4f, train err = %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_err\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainx_unl2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: not enough arguments for format string"
     ]
    }
   ],
   "source": [
    "# //////////// perform training //////////////\n",
    "lr = 0.003\n",
    "for epoch in range(300):\n",
    "    begin = time.time()\n",
    "\n",
    "    # construct randomly permuted minibatches\n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    for t in range(trainx_unl.shape[0]/txs.shape[0]):\n",
    "        inds = rng.permutation(txs.shape[0])\n",
    "        trainx.append(txs[inds])\n",
    "        trainy.append(tys[inds])\n",
    "    trainx = np.concatenate(trainx, axis=0)\n",
    "    trainy = np.concatenate(trainy, axis=0)\n",
    "    trainx_unl = trainx_unl[rng.permutation(trainx_unl.shape[0])]\n",
    "    trainx_unl2 = trainx_unl2[rng.permutation(trainx_unl2.shape[0])]\n",
    "\n",
    "    # train\n",
    "    loss_lab = 0.\n",
    "    loss_unl = 0.\n",
    "    train_err = 0.\n",
    "    for t in range(nr_batches_train):\n",
    "        ll, lu, te = train_batch_disc(trainx[t*args.batch_size:(t+1)*args.batch_size],trainy[t*args.batch_size:(t+1)*args.batch_size],\n",
    "                                        trainx_unl[t*args.batch_size:(t+1)*args.batch_size],lr)\n",
    "        loss_lab += ll\n",
    "        loss_unl += lu\n",
    "        train_err += te\n",
    "        print(\"Batch %d, train err = %.4f\" % (t, train_err,))\n",
    "\n",
    "        e = train_batch_gen(trainx_unl2[t*args.batch_size:(t+1)*args.batch_size],lr)\n",
    "    loss_lab /= nr_batches_train\n",
    "    loss_unl /= nr_batches_train\n",
    "    train_err /= nr_batches_train\n",
    "\n",
    "    # test\n",
    "    test_err = 0.\n",
    "    for t in range(nr_batches_test):\n",
    "        test_err += test_batch(testx[t*args.batch_size:(t+1)*args.batch_size],testy[t*args.batch_size:(t+1)*args.batch_size])\n",
    "    test_err /= nr_batches_test\n",
    "\n",
    "    # report\n",
    "    print(\"Iteration %d, time = %ds, loss_lab = %.4f, loss_unl = %.4f, train err = %.4f, test err = %.4f\" % (epoch, time.time()-begin, loss_lab, loss_unl, train_err, test_err))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
